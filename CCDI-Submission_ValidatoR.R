#!/usr/bin/env Rscript

#Childhood Cancer Data Initiative - Submission ValidatoR


##################
#
# USAGE
#
##################

#This takes a CCDI Metadata template file as input and creates an output file based on the QC checks.

#Run the following command in a terminal where R is installed for help.

#Rscript --vanilla CCDI-Submission_ValidatoR.R --help

##################
#
# Env. Setup
#
##################

#List of needed packages
list_of_packages=c("dplyr","tidyr","readr","stringi","janitor","readxl","openxlsx","optparse","tools")

#Based on the packages that are present, install ones that are required.
new.packages <- list_of_packages[!(list_of_packages %in% installed.packages()[,"Package"])]
suppressMessages(if(length(new.packages)) install.packages(new.packages, repos = "http://cran.us.r-project.org"))

#Load libraries.
suppressMessages(library(dplyr,verbose = F))
suppressMessages(library(readr,verbose = F))
suppressMessages(library(tidyr,verbose = F))
suppressMessages(library(stringi,verbose = F))
suppressMessages(library(janitor,verbose = F))
suppressMessages(library(readxl,verbose = F))
suppressMessages(library(openxlsx,verbose = F))
suppressMessages(library(optparse,verbose = F))
suppressMessages(library(tools,verbose = F))

#remove objects that are no longer used.
rm(list_of_packages)
rm(new.packages)


##################
#
# Arg parse
#
##################

#Option list for arg parse
option_list = list(
  make_option(c("-f", "--file"), type="character", default=NULL, 
              help="CCDI submission template workbook file (.xlsx, .tsv, .csv)", metavar="character"),
  make_option(c("-t", "--template"), type="character", default=NULL, 
              help="CCDI dataset template file, can be the same Metadata template workbook file or a blank CCDI_submission_metadata_template.xlsx", metavar="character"),
  make_option(c("-b", "--bucket_list"), type="character", default=NULL, 
              help="A bucket list file (tsv), generated by the Bucket_ls.py script. This option is best used when the script is run in a VM where connection to an aws account is not available.", metavar="character")
)

#create list of options and values for file input
opt_parser = OptionParser(option_list=option_list, description = "\nCCDI-Submission_ValidationR v1.1.2")
opt = parse_args(opt_parser)

#If no options are presented, return --help, stop and print the following message.
if (is.null(opt$file)&is.null(opt$template)){
  print_help(opt_parser)
  cat("Please supply both the input file (-f) and template file (-t), CCDI_submission_metadata_template.xlsx.\n\n")
  suppressMessages(stop(call.=FALSE))
}

#Addition of NULL option for the script as it runs through the CCDI_CDS_Pipeline.
if (!is.null(opt$bucket_list)){
  if (opt$bucket_list=="NO_LIST_PULL_FROM_S3"){
    opt$bucket_list=NULL
  }
}

#Data file pathway
file_path=file_path_as_absolute(opt$file)

#Template file pathway
template_path=file_path_as_absolute(opt$template)

#Bucket listing
if (!is.null(opt$bucket_list)){
  bucket_list_path=file_path_as_absolute(opt$bucket_list)
}

#A start message for the user that the validation is underway.
cat("The data file is being validated at this time.\n")


###############
#
# Start write out
#
###############

#Rework the file path to obtain a file name, this will be used for the output file.
file_name=stri_reverse(stri_split_fixed(stri_reverse(basename(file_path)),pattern = ".", n=2)[[1]][2])
ext=tolower(stri_reverse(stri_split_fixed(stri_reverse(basename(file_path)),pattern = ".", n=2)[[1]][1]))
path=paste(dirname(file_path),"/",sep = "")

#Output file name based on input file name and date/time stamped.
output_file=paste(file_name,
                  "_Validate",
                  stri_replace_all_fixed(
                    str = Sys.Date(),
                    pattern = "-",
                    replacement = ""),
                  sep="")

#Start writing in the outfile.
sink(paste(path,output_file,".txt",sep = ""))

cat(paste("This is a validation output for ",file_name,".\n----------",sep = ""))

###############
#
# Expected sheets to check
#
###############

#Pull sheet names
sheet_names=excel_sheets(path = template_path)
sheet_gone=FALSE

#Expected sheet names for a basic template
expected_sheets=c("Dictionary",             
                  "Terms and Value Sets")

#Test to see if expected sheet names are present.
if (all(expected_sheets%in%sheet_names)){
  cat("\n\tPASS: Found expected sheets in the submitted template file.\n")
}else{
  sheet_gone=TRUE
}

#If any sheet is missing, throw an overt error message then stops the process. This script pulls information from the expected sheets and requires all sheets present before running.
template_warning="\n\n################################################################################################################################\n#                                                                                                                              #\n# ERROR: Please obtain a new data template with all sheets and columns present before making further edits to this one.        #\n#                                                                                                                              #\n################################################################################################################################\n\n\n"

if (sheet_gone==TRUE){
  stop(paste("\nThe following sheet(s) is/are missing in the template file: ",paste(expected_sheets[!expected_sheets%in%sheet_names],collapse = ", "), template_warning, sep = ""), call.=FALSE)
}


##############
#
# Pull Dictionary Page to create node pulls
#
##############

#Read in Dictionary page to obtain the required properties.
df_dict=suppressMessages(read.xlsx(xlsxFile = template_path,sheet = "Dictionary"))
df_dict=remove_empty(df_dict,c('rows','cols'))

#Look for all entries that have a value
all_properties=unique(df_dict$Property)[grep(pattern = '.',unique(df_dict$Property))]
#Remove all entries that are all spaces
all_properties=all_properties[!grepl(pattern = " ",x = all_properties)]
#Pull out required property groups
required_property_groups=unique(df_dict$Required[!is.na(df_dict$Required)])
required_properties=df_dict$Property[!is.na(df_dict$Required)]
#Pull out nodes to read in respective tabs
dict_nodes=unique(df_dict$Node)


################
#
# Read in TaVS page to create value checks
#
################

#Read in Terms and Value sets page to obtain the required value set names.
df_tavs=suppressMessages(read.xlsx(xlsxFile = template_path, sheet = "Terms and Value Sets"))
df_tavs=remove_empty(df_tavs,c('rows','cols'))

#Pull out the positions where the value set names are located
VSN=unique(df_tavs$Value.Set.Name)
VSN=VSN[!is.na(VSN)]

df_all_terms=list()

#Pull the list of values for each controlled vocabulary property.
for (VSN_indv in VSN){
    df_all_terms[[VSN_indv]] = list(filter(df_tavs,Value.Set.Name==VSN_indv)["Term"][[1]])
  }


##############
#
# Read in each tab and apply to a data frame list
#
##############

cat("\n\nReading in the Metadata template workbook.\n----------")

# A bank of NA terms to make sure NAs are brought in correctly
NA_bank=c("NA","na","N/A","n/a")

#Establish the list
workbook_list=list()

#create a list of all node pages with data
for (node in dict_nodes){
  #read the sheet
  df=readWorkbook(xlsxFile = file_path,sheet = node, na.strings = NA_bank)
  #create an emptier version that removes the type and makes everything a character
  df_empty_test=df%>%
    select(-type)%>%
    mutate(across(everything(), as.character))
  #remove empty rows and columns
  df_empty_test=remove_empty(df_empty_test,c("rows","cols"))
  
  #if there are at least one row in the resulting data frame, add it
  if (dim(df_empty_test)[1]>0){
    #if the only columns in the resulting data frame are only linking properties (node.node_id), do not add it.
    if (any(!grepl(pattern = "\\.",x = colnames(df_empty_test)))){
      #add the data frame to the workbook
      df_typeless=df%>%
        select(-type)
      workbook_list=append(x = workbook_list,values = list(df_typeless))
      names(workbook_list)[length(workbook_list)]<-node
    }else{
      cat("\n\tWARNING: The following node, ", node,", did not contain any data except a linking value and type.\n" ,sep = "")
    }
  }
}

nodes_present=names(workbook_list)

####################
#
# Write out for validation
#
####################


##################
#
# Required property completeness and white space
#
##################

cat("\n\nThis section is for required properties for all nodes that contain data.\nFor information on required properties per node, please see the 'Dictionary' page of the template file.\nFor each entry, it is expected that all required information has a value:\n----------")

#For required columns in nodes, it will check if all required columns have values. If there are missing values or the values within the required column contain leading and/or trailing white space, it will return those row positions for the required columns.

for (node in nodes_present){
  cat("\n",node,"\n",sep = "")
  #initialize data frames and properties for tests
  df=workbook_list[node][[1]]
  properties=colnames(df)
  df_ws=df
  
  required_properties=df_dict$Property[grepl(pattern = TRUE,x = df_dict$Required %in% node)]
  if ("file_url_in_cds" %in% properties & "file_name" %in% properties & "file_size" %in% properties & "md5sum" %in% properties & "dcf_indexd_guid" %in% properties){
    file_req_props=c("file_name","file_size","file_type","md5sum","file_url_in_cds","dcf_indexd_guid")
    required_properties=unique(c(required_properties,file_req_props))
  }
  #initialize possible bad rows/cols
  bad_cols_all=c()
  
  if (!all(required_properties %in% colnames(df))){
    present_required_properties=required_properties[required_properties %in% colnames(df)]
    missing_required_properties=required_properties[!required_properties %in% colnames(df)]
    cat("\t####################################\n\tERROR: The following required columns are missing in this template: ", paste(missing_required_properties,collapse = ", "),".\n\tObtain a new template with the correct required properties present.\n\t####################################\n", sep = "")
    required_properties=present_required_properties
  }
  #If all required properties are present, continue test or skip and note that required properties are missing.
  df_req=df %>%
    select(all_of(required_properties))
  
  for (required_property in required_properties){
    
    #initialize possible bad rows/cols
    bad_rows=c()
    bad_cols_add=c()
    
    if (required_property %in% colnames(df)){
      error_title=FALSE
      bad_row_counter=0
      #check for rows that have any NA's or '' blank values
      bad_rows=grep(pattern = TRUE, x = !grepl(pattern = ".", x = df_req[required_property][[1]]))
      if (required_property=="dcf_indexd_guid" & length(bad_rows)==dim(df_req)[1]){
        
        #if it is a guid property
        bad_cols_add=grep(pattern = TRUE, x = colnames(df_req) %in% required_property)
        bad_cols_all=c(bad_cols_all,bad_cols_add)
        cat("\tERROR: For the node, ",node,", values still need to be generated for the required property, ", required_property,".\n", sep = "")
      }else if (length(bad_rows)>0){
        
        #if it is not a guid
        bad_cols_add=grep(pattern = TRUE, x = colnames(df_req) %in% required_property)
        bad_cols_all=c(bad_cols_all,bad_cols_add)
        
        #if all rows are bad, just call the entire property
        if (length(bad_rows)==dim(df_req)[1]){
          cat("\tERROR: The values for the node, ",node,", in the required property, ", required_property,", are missing.","\n", sep = "")
        }else{
          
          #if only some rows are bad
          for (bad_row in bad_rows){
            #if this is the first time through the extended information, output this line
            if (!error_title){
              cat("\tERROR: There are missing values for the node, ",node,", in the required property, ", required_property,", on the following rows: \n\t\t", sep = "")
              error_title=TRUE
              bad_row_indent_counter=0
            }
            #create a cleaner list, where there are line break to number lists.
            bad_row_indent_counter=bad_row_indent_counter+1
            #note the bad row
            cat(bad_row+1, sep = "")
            #if it is the last instance of the bad row, do a return for next property
            if (bad_row == bad_rows[length(bad_rows)]){
              cat('\n',sep = "")
              #reset counter to make sure no strange formats
              bad_row_indent_counter=0
              #otherwise, give a comma and output next row.
            }else{
              cat(", ", sep = "")
            }
            #if the counter hits the value, a new line and tabs will be made to keep the list organized, and the counter is reset.
            if (bad_row_indent_counter==25){
              cat("\n\t\t",sep = "")
              bad_row_indent_counter=0
            }
          }
        }
      }
    }else{
      missing_req_props=required_properties[!required_properties %in% colnames(df)]
      cat("\tERROR: For the node, ", node, " there are missing required columns: ",paste(missing_req_props,collapse = ", ",sep = ""),"\n",sep = "")
    }
  }
  
  
  #Check for white space in all values
  for (property in properties){
    
    #initialize possible bad rows/cols
    bad_rows=c()
    bad_cols_add=c()
    error_title=FALSE
    
    for (x in 1:dim(df[property])[1]){
      df_ws[property][x,]=trimws(df[property][x,])
    }
    
    #Return value positions that are either empty (NA) or contain leading/trailing white space in the value for the required column.
    if (!all(is.na(df_ws[property]))){
      if (!all(na.omit(df_ws[property]==df[property]))){
        bad_rows=grep(pattern = FALSE, x = df_ws[property]==df[property])
        if (property %in% required_properties){
          bad_cols_add=grep(pattern = TRUE, x = colnames(df_req) %in% property)
          bad_cols_all=c(bad_cols_all,bad_cols_add)
        }
        for (bad_row in bad_rows){
          if (!all(is.na(df[bad_row,property]))){
            
            #If there are bad rows, print them out
            if (!error_title){
              cat("\tERROR: For the node, ", node,", leading/trailing white space was found in the property, ",property,", on the following rows: \n\t\t", sep = "")
              error_title=TRUE
              bad_row_indent_counter=0
            }
            #create a cleaner list, where there are line break to number lists.
            bad_row_indent_counter=bad_row_indent_counter+1
            #note the bad row
            cat(bad_row+1, sep = "")
            #if it is the last instance of the bad row, do a return for next property
            if (bad_row == bad_rows[length(bad_rows)]){
              cat('\n',sep = "")
              #reset counter to make sure no strange formats
              bad_row_indent_counter=0
              #otherwise, give a comma and output next row.
            }else{
              cat(", ", sep = "")
            }
            #if the counter hits the value, a new line and tabs will be made to keep the list organized, and the counter is reset.
            if (bad_row_indent_counter==25){
              cat("\n\t\t",sep = "")
              bad_row_indent_counter=0
            }
          }
        }
      }
    }
  }
  
  #Summary of required columns that pass with no issue
  #Based on all the noted column issues, create a unique list of bad columns
  bad_cols_all=unique(bad_cols_all)
  
  if (length(bad_cols_all)<length(required_properties)){
    if(is.null(bad_cols_all)){
      pass_cols=required_properties
    }else{
      pass_cols=required_properties[-bad_cols_all]
    }
    for (pass_col in pass_cols){
      cat("\tPASS: For the node, ", node,", the required property, ",pass_col,", contains values for all expected entries.\n",sep = "")
    }
  }
}


##################
#
# Terms and Value sets checks
#
##################

cat("\n\nThe following columns have controlled vocabulary on the 'Terms and Value Sets' page of the template file:\n----------")

for (node in nodes_present){
  cat("\n",node,"\n",sep = "")
  #initialize data frames and properties for tests
  df=workbook_list[node][[1]]
  properties=colnames(df)
  #Enumerated Array properties
  enum_arrays=c('therapeutic_agents',"treatment_type","study_data_types","morphology","primary_site","race")
  
  #For the '_id' properties, make sure there are no illegal characters and it only has "Only the following characters can be included in the ID: English letters, Arabic numerals, period (.), hyphen (-), underscore (_), at symbol (@), and the pound sign (#)."
  for (property in properties){
    if (grepl(pattern = "_id", x = property)){
      bad_id_loc=grep(pattern = FALSE, x = grepl(pattern = '^[a-zA-Z0-9_.@#;-]*$', x = df[property][[1]]))
      if (length(bad_id_loc)>0){
        bad_cols_add=grep(pattern = TRUE, x = colnames(df_req) %in% property)
        bad_cols_all=c(bad_cols_all,bad_cols_add)
        for (bad_id in bad_id_loc){
          if (!is.na(df[property][[1]][bad_id])){
            cat(paste("\tERROR: The following ID, ",df[property][[1]][bad_id], ", has an illegal character (acceptable: A-z,0-9,_,.,-,@,#) in the property, ",property,".\n",sep = ""))
          }
        }
      }
    }
    if (property %in% names(df_all_terms)){
      if (property %in% enum_arrays){
        unique_values=unique(df[property][[1]])
        unique_values=unique(trimws(unlist(stri_split_fixed(str = unique_values,pattern = ";"))))
        unique_values=unique_values[!is.na(unique_values)]
        if (length(unique_values)>0){
          if (!all(unique_values%in%df_all_terms[property][[1]][[1]])){
            for (x in 1:length(unique_values)){
              check_value=unique_values[x]
              if (!is.na(check_value)){
                if (!as.character(check_value)%in%df_all_terms[property][[1]][[1]]){
                  cat(paste("\tERROR: ",property," property contains a value that is not recognized: ", check_value,"\n",sep = ""))
                }
              }
            }
          }else{
            cat(paste("\tPASS:",property,"property contains all valid values.\n"))
          }
        }
      }else{
        unique_values=unique(df[property][[1]])
        unique_values=unique_values[!is.na(unique_values)]
        if (length(unique_values)>0){
          if (!all(unique_values%in%df_all_terms[property][[1]][[1]])){
            for (x in 1:length(unique_values)){
              check_value=unique_values[x]
              if (!is.na(check_value)){
                if (!as.character(check_value)%in%df_all_terms[property][[1]][[1]]){3
                  #Look at Type for the Data Dictionary and break it apart if it is oneOf
                  oneOf_row=grep(pattern = TRUE, x = df_dict$Property %in% property)
                  oneOf_check=df_dict[oneOf_row,"Type"]
                  oneOf_check=stri_split_fixed(str = oneOf_check, pattern = ";")[[1]]
                  #if any part of the type is a string, then a warning will be thrown, instead of an error.
                  if (any(tolower(oneOf_check) %in% "string")){
                    cat(paste("\tWARNING: ",property," property contains a value that is not recognized in the permissible values, but does allow for free text strings: ", check_value,"\n",sep = ""))
                  }else{
                    cat(paste("\tERROR: ",property," property contains a value that is not recognized: ", check_value,"\n",sep = ""))
                  }
                }
              }
            }
          }else{
            cat(paste("\tPASS:",property,"property contains all valid values.\n"))
          }
        }
      }
    }
  }
}


################
#
# Integer and numeric checks
#
################

cat("\nThis section will display any values in properties that are expected to be either numeric or integer based on the Dictionary, but have values that are not:\n----------\n")

#Since the files are read in as "all strings" to ensure that the file can be ingested, this can hide issues with integers and numbers.
#This check will look at the dictionary to determine which properties should be integers and numbers and then force the strings into those types and make checks.

for (node in nodes_present){
  #initialize data frames and properties for tests
  df=workbook_list[node][[1]]
  df_colnames=colnames(df)
  
  num_properties=c()
  int_properties=c()
  
  #for each colname
  for (df_colname in df_colnames){
    #filter out linking properties.
    if (!grepl(pattern = "\\.", x = df_colname)){
      #if the filtered dictionary for that colname has a type equal to number
      if((df_dict%>%filter(Property == df_colname & Node == node))$Type == "number"){
        #add that colname to the df_dict_num list
        num_properties=c(num_properties,df_colname)
      }
      #if the filtered dictionary for that colname has a type equal to integer
      if((df_dict%>%filter(Property == df_colname & Node == node))$Type == "integer"){
        #add that colname to the df_dict_int list
        int_properties=c(int_properties,df_colname)
      }
    }
  }
 

  #NUMERIC CHECKS
  if (length(num_properties)>0){
    #for each number property
    for (num_prop in num_properties){
      error_title=TRUE
      
      #create empty vector for error rows
      error_rows=c()
      
      #go through each row
      for (row in 1:dim(df)[1]){
        if (!is.na(df[row,num_prop])){
          test_value=suppressWarnings(as.numeric(df[row,num_prop]))
          
          #if you cannot convert the value to a numeric, and check it is a number
          if (is.na(test_value)){
            #gather values to throw an error
            error_rows=c(error_rows,row)
          }
        }
      }
      if (length(error_rows)>0){
        error_title=FALSE
        
        if (!error_title){
          cat(paste("\n\tERROR: A non-numeric value was found in an expected numeric property row: ",node," - ",num_prop,"\n\t\t",sep = ""))
          error_title=TRUE
          bad_row_indent_counter=0
        }
        
        for (bad_row in error_rows){
          #create a cleaner list, where there are line break to number lists.
          bad_row_indent_counter=bad_row_indent_counter+1
          
          #note the bad row
          cat(bad_row+1, sep = "")
          
          #if it is the last instance of the bad row, do a return for next property
          if (bad_row == error_rows[length(error_rows)]){
            cat('\n',sep = "")
            #reset counter to make sure no strange formats
            bad_row_indent_counter=0
            #otherwise, give a comma and output next row.
          }else{
            cat(", ", sep = "")
          }
          #if the counter hits the value, a new line and tabs will be made to keep the list organized, and the counter is reset.
          if (bad_row_indent_counter==25){
            cat("\n\t\t",sep = "")
            bad_row_indent_counter=0
          }
        }
      }
    }
  }
  
  #INTEGER CHECKS
  if (length(int_properties)>0){
    #for int properties
    for (int_prop in int_properties){
      error_title=TRUE
      
      #create empty vector for error rows
      error_rows=c()
      
      #go through each row
      for (row in 1:dim(df)[1]){
        if (!is.na(df[row,int_prop])){
          
          #We need a different check as int are more structured and thus large ints cannot easily be converted from strings.
          #Thus we are going to double check via regex that the string is only made of numerals.
          int_test=grepl(pattern ="^[0-9-]+$" ,x = df[row,int_prop])
          
          if (int_test){
            test_value="actual_integer"
          }else{
            test_value=NA
          }
          
          
          #if you cannot convert the value to a integer, and check it is a integer
          if (is.na(test_value)){
            #gather values to throw an error
            error_rows=c(error_rows,row)
          }
        }
      }
      if (length(error_rows)>0){
        error_title=FALSE
        
        if (!error_title){
          cat(paste("\n\tERROR: A non-integer value was found in an expected integer property row: ",node," - ",int_prop,"\n\t\t",sep = ""))
          error_title=TRUE
          bad_row_indent_counter=0
        }
        
        for (bad_row in error_rows){
          #create a cleaner list, where there are line break to number lists.
          bad_row_indent_counter=bad_row_indent_counter+1
          
          #note the bad row
          cat(bad_row+1, sep = "")
          
          #if it is the last instance of the bad row, do a return for next property
          if (bad_row == error_rows[length(error_rows)]){
            cat('\n',sep = "")
            #reset counter to make sure no strange formats
            bad_row_indent_counter=0
            #otherwise, give a comma and output next row.
          }else{
            cat(", ", sep = "")
          }
          #if the counter hits the value, a new line and tabs will be made to keep the list organized, and the counter is reset.
          if (bad_row_indent_counter==25){
            cat("\n\t\t",sep = "")
            bad_row_indent_counter=0
          }
        }
      }
    }
  }
}




################
#
# Regex Checks
#
################

cat("\nThis section will display any values in properties that can accept strings, which are thought to contain PII/PHI based on regex suggestions from dbGaP:\n----------\n")

date_regex=c('(0?[1-9]|1[0-2])[-\\/.](0?[1-9]|[12][0-9]|3[01])[-\\/.](19[0-9]{2}|2[0-9]{3}|[0-9]{2})',
             '(19[0-9]{2}|2[0-9]{3})[-\\/.](0?[1-9]|1[0-2])[-\\/.](0?[1-9]|[12][0-9]|3[01])',
             '(0?[1-9]|[12][0-9]|3[01])[\\/](19[0-9]{2}|2[0-9]{3})',
             '(0?[1-9]|[12][0-9])[\\/]([0-9]{2})',
             '(0[1-9]|1[0-2])(0[1-9]|[12][0-9]|3[01])[0-9]{2}',
             '(0[1-9]|1[0-2])(0[1-9]|[12][0-9]|3[01])19[0-9]{2}',
             '(0[1-9]|1[0-2])(0[1-9]|[12][0-9]|3[01])2[0-9]{3}',
             '19[0-9]{2}(0[1-9]|1[0-2])(0[1-9]|[12][0-9]|3[01])',
             '2[0-9]{3}(0[1-9]|1[0-2])(0[1-9]|[12][0-9]|3[01])') 


#Problematic regex
#A month name or abbreviation and a 1, 2, or 4-digit number, in either order, separated by some non-letter, non-number characters or not separated, e.g., "JAN '93", "FEB64", "May 3rd" (but not "May be 14").
#```'[a-zA-Z]{3}[\ ]?([0-9]|[0-9]{2}|[0-9]{4})[a-zA-Z]{0,2}'```


socsec_regex=c('[0-9]{3}[-][0-9]{2}[-][0-9]{4}')
phone_regex=c('[(]?[0-9]{3}[-)\ ][0-9]{3}[-][0-9]{4}')
zip_regex=c('(^[0-9]{5}$)|(^[0-9]{9}$)|(^[0-9]{5}-[0-9]{4}$)')

string_df=df_dict[tolower(df_dict$Type) %in% "string",]

for (node in unique(string_df$Node)){
  if (node %in% names(workbook_list)){
    string_props_node=filter(string_df, Node==node)
    string_props=string_props_node$Property
    
    #logic to remove both GUID and md5sum from the check, as these are random/semi-random strings that are created and would never have a date placed in them.
    if ("md5sum" %in% string_props){
      string_props=string_props[! (string_props %in% 'md5sum')]
    }
    
    if ("dcf_indexd_guid" %in% string_props){
      string_props=string_props[! (string_props %in% 'dcf_indexd_guid')]
    }
    
    for (string in string_props){
      error_title=FALSE
      string_values=unique(workbook_list[node][[1]][string][[1]])
      if(any(!is.na(string_values))){
        string_values=string_values[!is.na(string_values)]
        for (value in string_values){
          date_hit=sapply(date_regex, function(z) grep(pattern=z, x = value))
          date_hit=unlist(date_hit)
          socsec_hit=grep(pattern = socsec_regex, x = value)
          phone_hit=grep(pattern = phone_regex, x = value)
          zip_hit=grep(pattern = zip_regex, x = value)
          
          #if there is any hit
          if (any(length(date_hit)!=0 |
                  length(socsec_hit)!=0 | 
                  length(phone_hit)!=0 |
                  length(zip_hit)!=0)){
            
            #If there are bad rows, print them out
            if (!error_title){
              cat(paste('\n',node,"\n\tWARNING: The following property, ",string,", in the node, ",node ,", contains a value that could be interpreted as a date/social security number/phone number/zip code: \n\t\t",sep = ""))
              error_title=TRUE
              bad_row_indent_counter=0
            }
            #create a cleaner list, where there are line break to number lists.
            bad_row_indent_counter=bad_row_indent_counter+1
            #value location
            bad_row=grep(pattern = TRUE, x = string_values %in% value)
            #note the bad row
            cat(bad_row+1, sep = "")
            #if it is the last instance of the bad row, do a return for next property
            if (value == string_values[length(string_values)]){
              cat('\n',sep = "")
              #reset counter to make sure no strange formats
              bad_row_indent_counter=0
              #otherwise, give a comma and output next row.
            }else{
              cat(", ", sep = "")
            }
            #if the counter hits the value, a new line and tabs will be made to keep the list organized, and the counter is reset.
            if (bad_row_indent_counter==25){
              cat("\n\t\t",sep = "")
              bad_row_indent_counter=0
            }
          }
          # cat('\n')
        }
      }
    }
  }
}


#################
#
# Unique Key check
#
#################

cat("\n\nThe following will check for multiples of key values, which are expected to be unique.\nIf there are any unexpected values, they will be reported below:\n----------")

#for each node create a data frame to check
for (node in nodes_present){
  #initialize data frames and properties for tests
  df=workbook_list[node][[1]]
  properties=colnames(df)

  #obtain the key value property
  key_value=df_dict%>%
    filter(Node==node, Key=="TRUE")
  
  key_value_prop=key_value$Property
  
  
  #check to make sure the key value exists, is not only NA and is the same value if it was unique.
  if (key_value_prop %in% properties){
    if (any(!is.na(df[key_value_prop]))){
      if (dim(df[key_value_prop])[1] != dim(unique(df[key_value_prop]))[1]){
        cat("\n",node,"\n",sep = "")
        cat("\tERROR: The following node, ", node, ", has multiple instances of the same key value, which should be unique, in the property, ", key_value_prop,":\n",sep = "")
        id_table=as.data.frame(table(df[key_value_prop][[1]]))
        id_table_gtr1=id_table%>%
          filter(Freq>1)
        cat("\t\t",paste(id_table_gtr1$Var1,"\n\t\t",collapse = "",sep = ""),sep = "")
      }
    }else {
      cat("\n",node,"\n",sep = "")
      cat("\tERROR: The following node, ", node, ", has no key values in the property, ", key_value_prop,".\n",sep = "")
    }
  }
}


#################
#
# Library to sample check
#
#################

cat("\n\nThis submission and subsequent submission files derived from this template assume that a library_id is associated to only one sample_id.\nIf there are any unexpected values, they will be reported below:\n----------")

#obtain df with libraries and samples
df=workbook_list['sequencing_file'][[1]]
library_id_list=unique(df$library_id)

error_title=FALSE
#For each library_id check to see how many instances it is found.
for (library_id in unique(df$library_id)){
  if(!is.na(library_id)){
    grep_instances=unique(df$sample.sample_id[grep(pattern = TRUE, x = df$library_id %in% library_id)])
    if (length(grep_instances)>1){
      
      #If there are bad rows, print them out
      if (!error_title){
        cat(paste("\nERROR: A library_id has multiple samples associated with it.\n\tThis setup will cause issues when submitting to SRA.\n",sep = ""))
        error_title=TRUE
      }
      
      #note the library_id and associated samples
      cat("\n\tlibrary_id: ",library_id,"\n\t\tsample.sample_id: ",paste(grep_instances,collapse = ", ",sep = ""), sep = "")

    }
  }
}


#################
#
# Require certain properties based on the file type.
#
#################

#For BAM, CRAM and Fastq files, we expect that all the files to have only one sample associated with them and the following properties: avg_read_length, coverage, bases, reads.

cat("\n\nThis submission and subsequent submission files derived from the sequencing file template assume that FASTQ, BAM and CRAM files are single sample files, and contain all associated metadata for submission.\nIf there are any unexpected values, they will be reported below:\n----------")


#Gather all file types.
file_types=c("fasta","fastq","bam","cram")

#sequencing nodes
seq_nodes=c()
for (node in nodes_present){
  node_cols=names(workbook_list[node][[1]])
  if (all('library_id' %in% node_cols & 'library_selection' %in% node_cols)){
    seq_nodes=c(seq_nodes, node)
  }
}

for (node in seq_nodes){
  #print node
  cat("\n",node,"\n",sep = "")
  
  #obtain df for seq files
  df=workbook_list[node][[1]]
  #library_id_list=unique(df$library_id)
  
  prob_sample_id_locs=c()
  prob_file_locs=c()
  
  
  #For each position, check to see if there are any samples that share the same library_id and make sure that the values for the required properties for SRA submission are present.      
  for (file_type in file_types){
    #determine if that file type is present
    single_sample_seq_files=grep(pattern = TRUE, x = tolower(df$file_type) %in% file_type)
    #as long as there is a value.
    if (length(single_sample_seq_files)>0){
      #print file type
      cat("\n\t",file_type,sep = "")
      
      #handle multiple files for a single sample
      error_title=FALSE
      for (file_location in single_sample_seq_files){
        sample_id=unique(df$sample.sample_id[file_location])
        
        sample_id_loc=unique(grep(pattern = TRUE, x = (df$sample.sample_id %in% sample_id)))
        sample_id_loc=sample_id_loc[grep(pattern=TRUE, x = (df$file_type[sample_id_loc] %in% file_types))]
        
        if (length(sample_id_loc)>1){
          #If there are multiple file for a sample, print them out
          if (!error_title){
            cat(paste("\n\t\tWARNING: A sample has multiple single sample files associated with it. These could cause errors in SRA submissions if this is unexpected.",sep = ""))
            error_title=TRUE
          }
          
          #if the sample is not already found in the problem samples (to remove redundancy)
          if (!any(sample_id_loc %in% prob_sample_id_locs)){
            cat("\n\t\t\tsample_id: ", sample_id, "\n\t\t\t\tfile_names: ",paste(df$file_name[sample_id_loc],collapse = ", ",sep = ""),sep = "")
          }
          prob_sample_id_locs=c(prob_sample_id_locs, sample_id_loc)
        }
      }
      
      
      #handle multiple files and file locations (urls)
      error_title=FALSE
      for (file_location in single_sample_seq_files){
        
        #find file name and url, plus possible other locations
        file_url=unique(df$file_url_in_cds[file_location])
        file_name=unique(df$file_name[file_location])
        file_url_loc=unique(grep(pattern = TRUE, x = (df$file_url_in_cds %in% file_url)))
        file_name_loc=unique(grep(pattern = TRUE, x = (df$file_name %in% file_name)))
        
        #if there are multiple positions for any urls or files 
        if (any(length(file_url_loc)>1 | length(file_name_loc)>1)){
          
          #If there are multiple files or urls, print them out
          if (!error_title){
            cat(paste("\n\t\tWARNING: The following file was found multiple times and there is a possibility that a file was found at different file urls.",sep = ""))
            error_title=TRUE
          }
          
          #if the file is not found already in the problem files (to remove redundancy)
          if (!any(file_name_loc %in% prob_file_locs)){
            cat("\n\t\t\tfile_name: ", file_name,sep = "")
            file_url_file_loc=unique(df$file_url_in_cds[file_name_loc])
            prob_file_locs=c(prob_file_locs,file_name_loc)
            
            #If there are multiple url locations to a file, it will note them
            if(length(file_url_file_loc)>1){
              cat("\n\t\t\t\turls: ",paste(file_url_file_loc,collapse = ", "), sep = "")
            }
          }
        }  
      }
      
      
      
      #handle SRA expected data checks
      error_title=FALSE
      for (file_location in single_sample_seq_files){
        #Check to see if the expected SRA metadata is present for the files going to the SRA submission.
        bases_check= df$number_of_bp[file_location]
        avg_read_length_check=df$avg_read_length[file_location]
        coverage_check=df$coverage[file_location]
        reads_check=df$number_of_reads[file_location]
        #for fastq files, skips the checks for coverage values to be present
        if (file_type=="fastq"){
          SRA_checks=c(bases_check, avg_read_length_check, reads_check)
          if (any(is.na(SRA_checks))){
            cat(paste("\n\t\tERROR: The file, ",df$file_name[file_location],", is missing at least one expected value (bases, avg_read_length, number_of_reads) that is associated with an SRA submission.",sep = ""))
          }
          if (!is.na(coverage_check)){
            cat(paste("\n\t\tWARNING: The file, ",df$file_name[file_location],", is not expected to have a coverage value.",sep = ""))
          }
          #for RNA-seq data, skips the checks for coverage values to be present
        }else if(!is.na(df$library_strategy[file_location]) &
                 tolower(df$library_strategy[file_location])=="rna-seq"){
          SRA_checks=c(bases_check, avg_read_length_check, reads_check)
          if (any(is.na(SRA_checks))){
            cat(paste("\n\t\tERROR: The file, ",df$file_name[file_location],", is missing at least one expected value (bases, avg_read_length, number_of_reads) that is associated with an SRA submission.\n",sep = ""))
          }
          if (!is.na(coverage_check)){
            cat(paste("\n\t\tWARNING: The file, ",df$file_name[file_location],", is not expected to have a coverage value.",sep = ""))
          }
        }else{
          SRA_checks=c(bases_check, avg_read_length_check, coverage_check, reads_check)
          if (any(is.na(SRA_checks))){
            cat(paste("\n\t\tERROR: The file, ",df$file_name[file_location],", is missing at least one expected value (bases, avg_read_length, coverage, number_of_reads) that is associated with an SRA submission.",sep = ""))
          }
          
        }
      }
    }
  }
  cat('\n')
}



#################
#
# File Checks
#
#################

cat("\n\nThe following section will compare the manifest against the reported buckets and note if there are unexpected results where the file is represented equally in both sources.\nIf there are any unexpected values, they will be reported below:\n----------\n")

#Pull out all nodes that have the file url, denoting that there are files in the node.
node_props=names(unlist(x = workbook_list, recursive = FALSE))
file_nodes=node_props[grep(pattern = "file_url_in_cds", x = node_props)]
file_nodes=unique(unlist(stri_split_fixed(str = file_nodes, pattern = ".",n = 2)))
file_nodes=file_nodes[!grepl(pattern = "file_url_in_cds", x = file_nodes)]
df_file=data.frame(matrix(ncol = 5,nrow = 0))
colnames(df_file)<-c("file_name","file_size","md5sum","file_url_in_cds","type")

for (node in file_nodes){
  #obtain df for files
  df=workbook_list[node][[1]]
  df$type=node
  df=df%>%
    select(file_name,file_size, md5sum,file_url_in_cds,type)
  df_file=rbind(df_file,df)
}
  
  
#################
#
# Check file metadata
#
#################

for (row_pos in 1:dim(df_file)[1]){
  if (!is.na(df_file$file_size[row_pos])){
    if (df_file$file_size[row_pos]==0){
      cat(paste("\tWARNING: The file ",df_file$file_name[row_pos],", has a size value of 0. Please make sure that this is a correct value for the file.\n",sep = ""))
    }
  }
  if (!is.na(df_file$md5sum[row_pos])){
    if (!stri_detect_regex(str = df_file$md5sum[row_pos],pattern = '^[a-f0-9]{32}$',case_insensitive=TRUE)){
      cat(paste("\tERROR: The file ",df_file$file_name[row_pos],", has a md5sum value that does not follow the md5sum regular expression.\n",sep = ""))
    }
  }
  if (!is.na(df_file$file_url_in_cds[row_pos])){
    if (df_file$file_name[row_pos]!=basename(df_file$file_url_in_cds[row_pos])){
      cat(paste("\tERROR: The file ",df_file$file_name[row_pos],", has a file_name that does not match the file_name in the url.\n",sep = ""))
    }
  }else{
    cat(paste("\tERROR: The file ",df_file$file_name[row_pos],", has a blank url path in the 'file_url_in_cds' property.\n",sep = ""))
  }
}


###############
#
# AWS bucket file check
#
###############

#Obtain bucket information
df_bucket=select(df_file, file_url_in_cds)%>%
  separate(file_url_in_cds,into = c("s3","blank","bucket","the_rest"),sep = "/",extra = "merge")%>%
  select(-s3,-blank,-the_rest)
df_bucket=unique(df_bucket)

#Check to see if there is only one bucket associated with the submission. It is not required, but it is likely that there would only be one bucket.
if (dim(df_bucket)[1]>1){
  cat(paste("\tWARNING: There is more than one aws bucket that is associated with this metadata submission file:\n\t\t", paste(df_bucket$bucket, collapse = ", "),".\n",sep = ""))
  if (!is.null(opt$bucket_list)){
    cat(paste("\tWARNING: A bucket list was supplied, it will need to contain all the bucket's files in that one list.\n",sep = ""))
  }
}

#Do a list of the bucket and then check the file size and name against the metadata submission.
for (bucket_num in 1:dim(df_bucket)[1]){
  #pull bucket metadata
  if (is.null(opt$bucket_list)){
    metadata_files=suppressMessages(suppressWarnings(system(command = paste("aws s3 ls --recursive s3://", df_bucket[bucket_num,],"/",sep = ""),intern = TRUE)))
    
    #fix bucket metadata to have fixed delimiters of one space
    while (any(grepl(pattern = "  ",x = metadata_files))==TRUE){
      metadata_files=stri_replace_all_fixed(str = metadata_files,pattern = "  ",replacement = " ")
    }
    
    #Break bucket string into a data frame and clean up
    bucket_metadata=data.frame(all_metadata=metadata_files)
    bucket_metadata=separate(bucket_metadata, all_metadata, into = c("date","time","file_size","file_path"),sep = " ", extra = "merge")%>%
      select(-date, -time)%>%
      mutate(file_path=paste("s3://",df_bucket[bucket_num,],"/",file_path,sep = ""))
    
  }else if (!is.null(opt$bucket_list)){
    bucket_metadata=suppressMessages(read_tsv(file = bucket_list_path, col_names = F))
    colnames(bucket_metadata)<-c("file_size","file_path")
    bucket_metadata=mutate(bucket_metadata, file_path=paste("s3://",df_bucket[bucket_num,],"/",file_path,sep = ""))
  }
  
  bucket_metadata$file_size=as.character(bucket_metadata$file_size)
  df_bucket_specific=df_file[grep(pattern = df_bucket[bucket_num,], x = df_file$file_url_in_cds),]
  
  #For each row in the manifest for this bucket, check the contents of the bucket against the manifest.
  for (row in 1:dim(df_bucket_specific)[1]){
    #locate the file url
    file_name_loc=grep(pattern = TRUE, x = bucket_metadata['file_path'][[1]] %in% df_bucket_specific[row,'file_url_in_cds'][[1]])
    #if the file is found, find that file with the correct size
    if (length(file_name_loc)!=0){
      if (bucket_metadata[file_name_loc,'file_size']!=as.character(df_bucket_specific[row,'file_size'][[1]])){
        cat(paste("\tERROR: The following file does not have the same file size found in the AWS bucket: ", df_bucket_specific[row,'file_url_in_cds'][[1]],"\n\t\tBucket:",bucket_metadata[file_name_loc,'file_size']," != Manifest:",as.character(df_bucket_specific[row,'file_size'][[1]]),"\n", sep = ""))
      }
    }else{
      cat(paste("\tERROR: The following file is not found in the AWS bucket: ", df_bucket_specific[row,'file_url_in_cds'][[1]],"\n", sep = ""))
    }
  }
  
  #Finally, check the bucket against the manifest to determine if there are files in the bucket that are not noted in the manifest.
  for (bucket_file in bucket_metadata$file_path){
    bucket_value = bucket_file  %in% df_bucket_specific['file_url_in_cds'][[1]]
    if (!bucket_value){
      cat(paste("\tERROR: The following file is found in the AWS bucket and not the manifest that was provided: ", bucket_file,"\n", sep = ""))
    }
  }
}


###############
#
# Cross node validation (do linking values have corresponding values)
#
###############

cat("\n\nIf there are unexpected or missing values in the linking values between nodes, they will be reported below:\n----------")

for (node in nodes_present){
  #note node, create df and pull out linking values
  cat("\n",node,"\n",sep = "")
  df=workbook_list[node][[1]]
  link_props_pos=grep(pattern = "\\.", x = colnames(df))
  link_props=colnames(df)[link_props_pos]
  
  #test for multiple links on different nodes
  if (length(link_props)>1){
    for (row in 1:dim(df)[1]){
      num_of_links=length(grep(pattern = TRUE, x = !is.na(df[row,link_props_pos])))
      if (num_of_links>1){
        cat("\tWARNING: For the node, ", node,", there are multiple links on row: ",row+1,"\n\t\tWhile multiple links can occur, they are often not needed or best practice.\n", sep="")
      }
    }
  }
  
  #for each linking value check to make sure there are values
  for (link_prop in link_props){
    link_values=unique(df[link_prop][[1]])
    link_values=link_values[!is.na(link_values)]
    
    #if there are values, then check values in the linking node
    if (length(link_values)>0){
      linking_node= stri_split_fixed(str = link_prop,pattern = ".",n = 2)[[1]][1]
      linking_prop=stri_split_fixed(str = link_prop,pattern = ".",n = 2)[[1]][2]
      df_link=workbook_list[linking_node][[1]]
      linking_values=unique(df_link[linking_prop][[1]])
      
      #if there an array in the linking values, break them apart to handle each one independently.
      for (link_value in link_values){
        if (grepl(pattern = ";", x = link_value)){
          value_split= unlist(stri_split_fixed(str = link_value, pattern = ";"))
          link_values=c(link_values,value_split)
          link_values=link_values[!(link_values %in% link_value)]
        }
        link_values=unique(link_values)
      }
      
      matching_links= link_values %in% linking_values
      
      #if not all the values match, determine the mismatched values
      if (!all(matching_links)){
        mis_match_value=grep(pattern = FALSE, x = matching_links)
        
        #for each mismatched value, throw an error.
        for (mis_match in mis_match_value){
          mis_match_text=link_values[mis_match]
          cat("\tERROR: For the node, ",node,", the following linking property, ", link_prop,", has a value that is not found in the parent node: ", mis_match_text,"\n",sep = "")
        }
      }else{
        cat("\tPASS: The links for the node, ", node,", have corresponding values in the parent node, ",linking_node,".\n", sep = "")
      }
    }
  }
}


#################
#
# Stop write out
#
#################

#Stop write out to file and display "done message" on command line.
sink()

cat(paste("\n\nProcess Complete.\n\nThe output file can be found here: ",path,"\n\n",sep = "")) 
